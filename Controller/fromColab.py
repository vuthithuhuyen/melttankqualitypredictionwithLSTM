# -*- coding: utf-8 -*-
"""Melt tank.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QXE1cktuBdNELkP5LiBYaVMBk1zxdfvj
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
from datetime import datetime
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import *
from imblearn.over_sampling import SMOTE

from keras.models import Sequential
from keras.layers import Dense, LSTM
from keras.preprocessing import sequence
from keras.utils import np_utils
from keras.callbacks import EarlyStopping, ModelCheckpoint

from Model.GlobalVariables import raw_data_file

filePath = raw_data_file
df = pd.read_csv(filePath, encoding='cp949')

df.head()

df.describe()

# df['STD_DT'] = df['STD_DT'].apply(lambda x: pd.to_datetime(str(x),format='%Y-%m-%d %H:%M:%S'))

tagKey = ['OK', 'NG']
for key in tagKey:
  print(f"{key} - percentage: {df['TAG'].value_counts()[key]/len(df['TAG'])}")

# df = df.set_index(['STD_DT'])
df

df.head(20)

"""### 2-3 histogram 및 plot 그리기"""

col_name = ['MELT_TEMP', 'MOTORSPEED', 'MELT_WEIGHT', 'INSP', 'TAG']
plt.figure(figsize=(12, 12))
list_num = []
for i in range(len(col_name)):
    num = 231+i
    list_num.append(num)
    plt.subplot(num)
    plt.hist(df[col_name[i]])
    plt.xticks(rotation=45)
    plt.title(col_name[i])
plt.show()

"""#### 관측치의 패턴 확인을 위한 plot 그리기"""

col_name = ['MELT_TEMP', 'MOTORSPEED', 'MELT_WEIGHT', 'INSP']
plt.figure(figsize=(12, 12))
for i in range(len(col_name)):
    num = 221+i
    plt.subplot(num)
    plt.plot(df[col_name[i]])
    plt.xticks(rotation=45)
    plt.title(col_name[i])
plt.subplots_adjust(left=0.125, bottom=0.1,  right=0.9, top=0.9, wspace=0.2, hspace=0.35)
plt.show()

plt.figure(figsize=(12, 12))
for i in range(len(col_name)):
    num = 221+i
    plt.subplot(num)
    plt.plot(df[col_name[i]][0:30])
    plt.xticks(rotation=45)
    plt.title(col_name[i])
plt.subplots_adjust(left=0.125, bottom=0.1,  right=0.9, top=0.9, wspace=0.2, hspace=0.35)
plt.show()

encoder = preprocessing.LabelEncoder()
df['TAG'] = encoder.fit_transform(df['TAG'])  # 범주형 변수 숫자로 인코딩
df['TAG'] = df['TAG'].astype('float32')

df['TAG'].value_counts()

"""### 2-5. 상관 분석"""

corr = df.corr(method = 'pearson')
corr

df1 = df[['MELT_TEMP', 'MOTORSPEED', 'TAG']]

df1

split_date = int(df1.shape[0]*0.7)   # df.shape[0]*0.7 = 584640
train = df1[:split_date]
test = df1[split_date:]

print(train.shape)
print(test.shape)

"""### 5-1. MinMax 정규화"""

scaler = preprocessing.MinMaxScaler()

train_sc = scaler.fit_transform(train)
test_sc = scaler.transform(test)

train_sc

X_train_values = train_sc[:, :-1]
y_train_values = train_sc[:,-1]
print(X_train_values, y_train_values)

"""### 5-2. 클래스 불균형 문제 해결(SMOTE)"""

smote = SMOTE(random_state=0)

X_train_over,y_train_over = smote.fit_resample(X_train_values,y_train_values)
print('SMOTE Feature/label dataset for training before application: ', X_train_values.shape, y_train_values.shape)
print('SMOTE Feature/label dataset for training after application: ', X_train_over.shape, y_train_over.shape)
print('SMOTE Distribution of label values after application: \n', pd.Series(y_train_over).value_counts())

"""### 6-1. Window 정의 함수 생성"""

def make_dataset(data, label, window_size):
    feature_list = []
    label_list = []
    for i in range(len(data) - window_size):
        feature_list.append(np.array(data.iloc[i:i+window_size]))
        label_list.append(np.array(label.iloc[i+window_size]))
    return np.array(feature_list), np.array(label_list)

"""### 6-2. window 정의 함수를 활용한 데이터 생성"""

X_train_over = pd.DataFrame(X_train_over, columns=['MELT_TEMP', 'MOTORSPEED'])
y_train_over = pd.DataFrame(y_train_over, columns=['TAG'])
print(X_train_over, y_train_over)

# train dataset
train_feature, train_label = make_dataset(X_train_over, y_train_over, 10)
print(train_feature, train_label)

print(train_feature.shape, train_label.shape)
print(test_sc)

test_sc = pd.DataFrame(test_sc, columns=['MELT_TEMP', 'MOTORSPEED', 'TAG'])

feature_cols = ['MELT_TEMP', 'MOTORSPEED']
label_cols = ['TAG']

test_feature = test_sc[feature_cols]
test_label = test_sc[label_cols]

# test dataset (실제 예측 해볼 데이터)
test_feature, test_label = make_dataset(test_feature, test_label, 10)
test_feature.shape, test_label.shape

"""### 6-3. 검증 데이터(Validation data) 생성

모델의 평가를 위하여 훈련데이터를 모델 학습을 위한 훈련 데이터와 모델 검증(validation) 데이터로 나눔
"""

# train, validation set 생성
x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.3)

x_train.shape, x_valid.shape

"""## 단계7. 모델 구축 및 훈련

### 7-1. LSTM 모델 구축
"""

model = Sequential()
model.add(LSTM(50,
               input_shape=(train_feature.shape[1], train_feature.shape[2]),
               activation='tanh',
               return_sequences=False)
          )
model.add(Dense(1, activation='sigmoid'))

"""### 7-2. 모델 훈련"""

model_path=os.getcwd()
model_path

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
early_stop = EarlyStopping(monitor='val_loss', patience=5)
filename = os.path.join(model_path, 'tmp_checkpoint.h5')
checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=0,
                             save_best_only=True, mode='auto')

history = model.fit(x_train, y_train,
                    epochs=2,
                    batch_size=50,
                    validation_data=(x_valid, y_valid)
                    ,callbacks=[early_stop, checkpoint])

"""### 7.3. 테스트 데이터를 활용한 예측"""

model.load_weights(filename)

pred = model.predict(test_feature)

print(pred)

plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='validation')
plt.legend()
plt.show()

pred_df = pd.DataFrame(pred, columns=['TAG'])

pred_df['TAG'] = pred_df['TAG'].apply(lambda x: 1 if x >= 0.5 else 0)

pred_df['TAG'].value_counts()

classify=confusion_matrix(test_label, pred_df)
print(classify)

p = precision_score(test_label, pred_df)
print("precision: %0.4f" %p)
r = recall_score(test_label, pred_df)
print("recall: %0.4f" %r)
f1 = f1_score(test_label, pred_df)
print("f1-score: %0.4f" %f1)
acc = accuracy_score(test_label, pred_df)
print("accuracy: %0.4f" %acc)

